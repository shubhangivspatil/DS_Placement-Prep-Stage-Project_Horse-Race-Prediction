{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7648c30d-c48e-40a2-ac47-4871f3c8bc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Directory containing all the files\n",
    "data_dir = 'D:/GUVI_Projects/My_Projects/new_horse/Horse'\n",
    "\n",
    "# Function to load and check columns of horse and race data for a given year\n",
    "def load_and_check_columns(year, data_dir):\n",
    "    horse_file = os.path.join(data_dir, f'horses_{year}.csv')\n",
    "    race_file = os.path.join(data_dir, f'races_{year}.csv')\n",
    "    \n",
    "    if os.path.exists(horse_file) and os.path.exists(race_file):\n",
    "        horses = pd.read_csv(horse_file, low_memory=False)\n",
    "        races = pd.read_csv(race_file, low_memory=False)\n",
    "        \n",
    "        print(f\"Columns in horses_{year}.csv: {horses.columns}\")\n",
    "        print(f\"Columns in races_{year}.csv: {races.columns}\")\n",
    "    else:\n",
    "        print(f\"Files for year {year} not found.\")\n",
    "\n",
    "# Check columns for each year from 1990 to 2020\n",
    "for year in range(1990, 2021):\n",
    "    load_and_check_columns(year, data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26db4a07-92a5-4f3a-a66c-add8bd41ddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Directory containing all the files\n",
    "data_dir = 'D:/GUVI_Projects/My_Projects/new_horse/Horse'\n",
    "\n",
    "# Function to load and merge horse and race data for a given year\n",
    "def load_and_merge_yearly_data(year, data_dir):\n",
    "    horse_file = os.path.join(data_dir, f'horses_{year}.csv')\n",
    "    race_file = os.path.join(data_dir, f'races_{year}.csv')\n",
    "    \n",
    "    if os.path.exists(horse_file) and os.path.exists(race_file):\n",
    "        horses = pd.read_csv(horse_file, low_memory=False)\n",
    "        races = pd.read_csv(race_file, low_memory=False)\n",
    "        \n",
    "        # Ensure 'rid' exists in both datasets\n",
    "        if 'rid' in horses.columns and 'rid' in races.columns:\n",
    "            merged_data = pd.merge(horses, races, on='rid')\n",
    "            print(f\"Successfully merged data for year {year}\")\n",
    "            return merged_data\n",
    "        else:\n",
    "            print(f\"'rid' column not found in one of the files for year {year}. Skipping...\")\n",
    "            return pd.DataFrame()\n",
    "    else:\n",
    "        print(f\"Files for year {year} not found. Skipping...\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame if the files don't exist\n",
    "\n",
    "# Load and merge data from 1990 to 2020\n",
    "all_data = pd.DataFrame()\n",
    "\n",
    "for year in range(1990, 2021):\n",
    "    yearly_data = load_and_merge_yearly_data(year, data_dir)\n",
    "    if not yearly_data.empty:\n",
    "        all_data = pd.concat([all_data, yearly_data], ignore_index=True)\n",
    "\n",
    "print(\"All yearly data loaded and merged.\")\n",
    "print(\"Columns in all_data:\", all_data.columns)\n",
    "\n",
    "# Load forward.csv\n",
    "forward_file = os.path.join(data_dir, 'forward.csv')\n",
    "forward_data = pd.read_csv(forward_file, low_memory=False)\n",
    "print(\"Forward data loaded.\")\n",
    "print(\"Columns in forward_data:\", forward_data.columns)\n",
    "\n",
    "# Ensure necessary columns exist in both datasets before merging\n",
    "merge_columns = ['course', 'title', 'rclass', 'horseName', 'trainerName', 'jockeyName']\n",
    "if all(col in all_data.columns for col in merge_columns) and all(col in forward_data.columns for col in merge_columns):\n",
    "    complete_data = pd.merge(all_data, forward_data, on=merge_columns, how='left')\n",
    "    print(\"Forward data merged with all yearly data.\")\n",
    "else:\n",
    "    print(\"One of the necessary columns not found in one of the datasets. Merging skipped.\")\n",
    "    complete_data = all_data\n",
    "\n",
    "# Select important columns that are present in the DataFrame\n",
    "important_columns = [\n",
    "    'rid', 'horseName', 'trainerName', 'jockeyName', 'date', 'course', 'age', 'weight', \n",
    "    'distance', 'condition', 'position', 'decimalPrice', 'RPR', 'TR', 'OR'\n",
    "]\n",
    "existing_columns = [col for col in important_columns if col in complete_data.columns]\n",
    "complete_data = complete_data.loc[:, existing_columns]\n",
    "print(\"Selected important columns.\")\n",
    "\n",
    "# Handle missing values\n",
    "complete_data.ffill(inplace=True)\n",
    "print(\"Handled missing values using forward fill.\")\n",
    "\n",
    "# Convert categorical variables to numerical representations\n",
    "categorical_columns = ['course', 'trainerName', 'jockeyName']\n",
    "for column in categorical_columns:\n",
    "    if column in complete_data.columns:\n",
    "        complete_data[column] = complete_data[column].astype('category').cat.codes\n",
    "print(\"Encoded categorical variables to numerical representations.\")\n",
    "\n",
    "# Save the final cleaned dataset\n",
    "output_file = os.path.join(data_dir, 'cleaned_final_dataset.csv')\n",
    "complete_data.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"Dataset creation completed and saved as 'cleaned_final_dataset.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e250c452-44ec-4fb3-9d61-468cd58235cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('D:/GUVI_Projects/My_Projects/new_horse/Horse/cleaned_final_dataset.csv')\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6037d6-827b-4b01-b168-1590a456693d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.isnull().sum())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295f1a63-b65c-4e2c-b6fc-2e4b03b239f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to create and display histograms\n",
    "def plot_histogram(data, column, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(data[column], bins=30, edgecolor='black', alpha=0.7)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# List of columns to plot\n",
    "columns_to_plot = ['weight', 'position', 'RPR', 'TR']\n",
    "\n",
    "# Plot histograms for the specified columns\n",
    "for column in columns_to_plot:\n",
    "    plot_histogram(data, column, f'Distribution of {column.capitalize()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1dfc30-2eef-4797-9f23-679cf612c8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Selecting only numeric columns for the correlation matrix\n",
    "numeric_data = data.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Creating the correlation matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(numeric_data.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8980a60-11c5-40b9-b90a-9374c5d0acd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "# Function to add value labels on bars\n",
    "def add_value_labels(ax):\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(format(p.get_height(), '.0f'),\n",
    "                   (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                   ha = 'center', va = 'center',\n",
    "                   xytext = (0, 10),\n",
    "                   textcoords = 'offset points')\n",
    "\n",
    "# Load the entire dataset\n",
    "df = pd.read_csv('D:/GUVI_Projects/My_Projects/new_horse/Horse/cleaned_final_dataset.csv', low_memory=True)\n",
    "\n",
    "# Ensure the columns are treated as strings\n",
    "df['course'] = df['course'].astype(str)\n",
    "df['trainerName'] = df['trainerName'].astype(str)\n",
    "df['jockeyName'] = df['jockeyName'].astype(str)\n",
    "\n",
    "# Count the occurrences of each unique value in the columns\n",
    "course_counts = df['course'].value_counts().reset_index()\n",
    "trainer_counts = df['trainerName'].value_counts().reset_index()\n",
    "jockey_counts = df['jockeyName'].value_counts().reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "course_counts.columns = ['course', 'count']\n",
    "trainer_counts.columns = ['trainerName', 'count']\n",
    "jockey_counts.columns = ['jockeyName', 'count']\n",
    "\n",
    "# Example mapping dictionaries (replace with actual mappings if available)\n",
    "trainer_name_mapping = {9907: \"Trainer A\", 15408: \"Trainer B\", 15396: \"Trainer C\"}\n",
    "jockey_name_mapping = {4783: \"Jockey A\", 4740: \"Jockey B\", 15750: \"Jockey C\"}\n",
    "\n",
    "# Replace IDs with names\n",
    "trainer_counts['trainerName'] = trainer_counts['trainerName'].astype(int).map(trainer_name_mapping).fillna(trainer_counts['trainerName'])\n",
    "jockey_counts['jockeyName'] = jockey_counts['jockeyName'].astype(int).map(jockey_name_mapping).fillna(jockey_counts['jockeyName'])\n",
    "\n",
    "# Bar plot for Course\n",
    "if not course_counts.empty:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    course_order = course_counts.sort_values('count', ascending=False).head(20)['course']\n",
    "    ax = sns.barplot(data=course_counts.head(20), x='course', y='count', order=course_order)\n",
    "    plt.xticks(rotation=90, fontsize=10)\n",
    "    plt.xlabel('Course')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Top 20 Courses by Count')\n",
    "    add_value_labels(ax)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available for 'course'.\")\n",
    "\n",
    "# Bar plot for Trainer Names (top 20 for better visualization)\n",
    "if not trainer_counts.empty:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_trainers = trainer_counts.nlargest(20, 'count')\n",
    "    trainer_order = top_trainers.sort_values('count', ascending=False)['trainerName']\n",
    "    ax = sns.barplot(data=top_trainers, y='trainerName', x='count', order=trainer_order)\n",
    "    plt.xticks(rotation=90, fontsize=10)\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel('Trainer Name')\n",
    "    plt.title('Top 20 Trainers')\n",
    "    add_value_labels(ax)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available for 'trainerName'.\")\n",
    "\n",
    "# Bar plot for Jockey Names (top 20 for better visualization)\n",
    "if not jockey_counts.empty:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_jockeys = jockey_counts.nlargest(20, 'count')\n",
    "    jockey_order = top_jockeys.sort_values('count', ascending=False)['jockeyName']\n",
    "    ax = sns.barplot(data=top_jockeys, y='jockeyName', x='count', order=jockey_order)\n",
    "    plt.xticks(rotation=90, fontsize=10)\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel('Jockey Name')\n",
    "    plt.title('Top 20 Jockeys')\n",
    "    add_value_labels(ax)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available for 'jockeyName'.\")\n",
    "\n",
    "# Additional Visualizations\n",
    "\n",
    "# Visualization for top 20 horse names by count\n",
    "horse_counts = df['horseName'].value_counts().reset_index()\n",
    "horse_counts.columns = ['horseName', 'count']\n",
    "\n",
    "if not horse_counts.empty:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_horses = horse_counts.nlargest(20, 'count')\n",
    "    horse_order = top_horses.sort_values('count', ascending=False)['horseName']\n",
    "    ax = sns.barplot(data=top_horses, x='horseName', y='count', order=horse_order)\n",
    "    plt.xticks(rotation=90, fontsize=10)\n",
    "    plt.xlabel('Horse Name')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Top 20 Horses by Count')\n",
    "    add_value_labels(ax)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available for 'horseName'.\")\n",
    "\n",
    "# Visualization for race positions\n",
    "position_counts = df['position'].value_counts().reset_index()\n",
    "position_counts.columns = ['position', 'count']\n",
    "\n",
    "if not position_counts.empty:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    position_order = position_counts.sort_values('count', ascending=False)['position']\n",
    "    ax = sns.barplot(data=position_counts, x='position', y='count', order=position_order)\n",
    "    plt.xticks(rotation=90, fontsize=10)\n",
    "    plt.xlabel('Position')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Race Positions by Count')\n",
    "    add_value_labels(ax)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available for 'position'.\")\n",
    "\n",
    "# Interactive Plots using Plotly\n",
    "\n",
    "# Interactive plot for top 20 courses by count\n",
    "fig = px.bar(course_counts.head(20), x='course', y='count', title='Top 20 Courses by Count')\n",
    "fig.show()\n",
    "\n",
    "# Interactive plot for top 20 trainers by count\n",
    "fig = px.bar(trainer_counts.head(20), x='trainerName', y='count', title='Top 20 Trainers by Count')\n",
    "fig.show()\n",
    "\n",
    "# Interactive plot for top 20 jockeys by count\n",
    "fig = px.bar(jockey_counts.head(20), x='jockeyName', y='count', title='Top 20 Jockeys by Count')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcca2c1-7534-49e7-984c-36ce362207f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering:\n",
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned dataset\n",
    "df = pd.read_csv('D:/GUVI_Projects/My_Projects/new_horse/Horse/cleaned_final_dataset.csv')\n",
    "\n",
    "# Example feature engineering: Creating features based on past performances\n",
    "df['win'] = (df['position'] == 1).astype(int)\n",
    "df['place'] = df['position'].apply(lambda x: 1 if x <= 3 else 0)\n",
    "\n",
    "# Aggregating performance metrics for horses\n",
    "horse_performance = df.groupby('horseName').agg({\n",
    "    'win': 'sum',\n",
    "    'place': 'sum',\n",
    "    'RPR': 'mean',\n",
    "    'TR': 'mean',\n",
    "    'weight': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Merging performance metrics back into the main dataset\n",
    "df = df.merge(horse_performance, on='horseName', suffixes=('', '_avg'))\n",
    "\n",
    "print(\"Feature engineering completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da0b578-9d9f-4832-9c8e-da3f5d334ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preprocessing for Modeling:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Selecting features and target variable\n",
    "features = ['weight', 'RPR', 'TR', 'win_avg', 'place_avg', 'RPR_avg', 'TR_avg', 'weight_avg']\n",
    "X = df[features]\n",
    "y = df['win']  # Assuming the goal is to predict if the horse wins\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scaling the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Data preprocessing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574f2cf8-4929-4d9b-a7ab-dbb8615db98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Selection and Training:\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Training a RandomForest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Random Forest Model Performance:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Random Forest Model Performance:\n",
    "# Accuracy: 0.9022526430432796\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.91      0.99      0.95   1113116\n",
    "#            1       0.47      0.08      0.14    119081\n",
    "\n",
    "#     accuracy                           0.90   1232197\n",
    "#    macro avg       0.69      0.54      0.55   1232197\n",
    "# weighted avg       0.87      0.90      0.87   1232197\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c75083c-c0ef-41a0-9013-83d0f4449e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handling Imbalanced Data:\n",
    "#Use techniques to handle imbalanced data, such as SMOTE.\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Applying SMOTE to the training data\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Retraining the model on balanced data\n",
    "rf_model_res = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model_res.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Predictions on original test data\n",
    "y_pred_res = rf_model_res.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Random Forest Model with SMOTE Performance:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_res))\n",
    "print(classification_report(y_test, y_pred_res))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix_res = confusion_matrix(y_test, y_pred_res)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(conf_matrix_res, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix with SMOTE')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Random Forest Model with SMOTE Performance:\n",
    "# Accuracy: 0.890455828085931\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.92      0.96      0.94   1113116\n",
    "#            1       0.38      0.22      0.28    119081\n",
    "\n",
    "#     accuracy                           0.89   1232197\n",
    "#    macro avg       0.65      0.59      0.61   1232197\n",
    "# weighted avg       0.87      0.89      0.88   1232197\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3d0d52-870c-4c6f-8499-3afc3ff6b2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter Tuning:\n",
    "#Optimize hyperparameters using Grid Search or Random Search.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Hyperparameter tuning using Grid Search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [None, 10],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Reduce the number of parallel jobs to avoid memory issues\n",
    "grid_search = GridSearchCV(estimator=rf_model_res, param_grid=param_grid, cv=3, n_jobs=1, verbose=2)\n",
    "grid_search.fit(X_train_res, y_train_res)\n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "# Retraining the model with best parameters\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "y_pred_best = best_rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Random Forest Model with Best Parameters Performance:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_best))\n",
    "print(classification_report(y_test, y_pred_best))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix_best = confusion_matrix(y_test, y_pred_best)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(conf_matrix_best, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix with Best Parameters')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Save the trained model and the final dataset for future use\n",
    "import joblib\n",
    "\n",
    "# Save the final dataset\n",
    "final_dataset_path = 'D:/GUVI_Projects/My_Projects/new_horse/Horse/final_dataset.csv'\n",
    "df.to_csv(final_dataset_path, index=False)\n",
    "print(\"Final dataset saved.\")\n",
    "\n",
    "# Save the trained model\n",
    "model_path = 'D:/GUVI_Projects/My_Projects/new_horse/Horse/best_rf_model.pkl'\n",
    "joblib.dump(best_rf_model, model_path)\n",
    "print(\"Trained model saved.\")\n",
    "\n",
    "# Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
    "# [CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=15.0min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8b55fd-5cc3-4260-990a-b2b2108a293e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ca6e01-ef26-4231-8c4b-635f61f08f70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
